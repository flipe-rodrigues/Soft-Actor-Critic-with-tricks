{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import mujoco\n",
    "import mujoco.viewer\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from agent import SACAgent\n",
    "from utils import ReplayBuffer\n",
    "from trainer import SACTrainer\n",
    "\n",
    "# This solve the pendulum environment with soft actor critic algorithm\n",
    "\n",
    "# It uses a bunch of tricks to make this work better:\n",
    "# Twin Q-Networks\n",
    "# Memory Replay (Experience Replay Buffer)\n",
    "# Target Networks with Polyak Averaging\n",
    "# Automatic Entropy Tuning\n",
    "# Reparameterization Trick\n",
    "# Tanh Action Squashing with Log-Probability Correction\n",
    "# Gradient Clipping\n",
    "# Random Action Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialReachingEnv(gym.Env):\n",
    "    \"\"\"Custom 2-Joint Limb with 4 Muscles, 12 Sensors, and a Target Position\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        xml_file=\"your_model.xml\",\n",
    "        max_num_targets=10,\n",
    "        max_target_duration=3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        mj_dir = \"../mujoco\"\n",
    "        xml_path = os.path.join(mj_dir, xml_file)\n",
    "        self.model = mujoco.MjModel.from_xml_path(xml_path)\n",
    "        self.data = mujoco.MjData(self.model)\n",
    "        self.max_num_targets = max_num_targets\n",
    "        self.max_target_duration = max_target_duration\n",
    "        self.viewer = None\n",
    "\n",
    "        # Get the site ID using the name of your end effector\n",
    "        self.hand_id = self.model.geom(\"hand\").id\n",
    "        \n",
    "        # Load sensor stats\n",
    "        sensor_stats_path = os.path.join(mj_dir, \"sensor_stats.pkl\")\n",
    "        with open(sensor_stats_path, \"rb\") as f:\n",
    "            self.sensor_stats = pickle.load(f)\n",
    "\n",
    "        # Load target stats\n",
    "        target_stats_path = os.path.join(mj_dir, \"target_stats.pkl\")\n",
    "        with open(target_stats_path, \"rb\") as f:\n",
    "            self.target_stats = pickle.load(f)\n",
    "\n",
    "        # Define the lower and upper bounds for each feature (15 features)\n",
    "        low_values = np.concatenate(\n",
    "            [\n",
    "                self.sensor_stats[\"min\"].values,\n",
    "                self.target_stats[\"min\"].values,\n",
    "            ]\n",
    "        )\n",
    "        high_values = np.concatenate(\n",
    "            [\n",
    "                self.sensor_stats[\"max\"].values,\n",
    "                self.target_stats[\"max\"].values,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Observation space: 12 sensor readings + 3D target position\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=low_values, high=high_values, dtype=np.float64\n",
    "        )\n",
    "\n",
    "        # Action space: 4 muscle activations\n",
    "        self.action_space = spaces.Box(low=0.0, high=1.0, shape=(4,), dtype=np.float64)\n",
    "\n",
    "        # Load valid target positions\n",
    "        reachable_positions_path = os.path.join(mj_dir, \"reachable_positions.pkl\")\n",
    "        with open(reachable_positions_path, \"rb\") as f:\n",
    "            self.reachable_positions = pickle.load(f)\n",
    "\n",
    "    def sample_targets(self, num_samples=10):\n",
    "        return self.reachable_positions.sample(num_samples).values\n",
    "\n",
    "    def update_target(self, position):\n",
    "        self.data.mocap_pos = position\n",
    "        mujoco.mj_forward(self.model, self.data)\n",
    "\n",
    "    def step(self, action):\n",
    "        self.data.ctrl[:] = action\n",
    "        mujoco.mj_step(self.model, self.data)\n",
    "\n",
    "        sensor_data = self.data.sensordata.copy()\n",
    "        hand_position = self.data.site_xpos[self.hand_id]\n",
    "        distance = np.linalg.norm(\n",
    "            hand_position - self.target_positions[self.target_idx]\n",
    "        )\n",
    "        reward = -distance\n",
    "\n",
    "        done = self.data.time > self.max_target_duration * self.max_num_targets\n",
    "        terminated = False\n",
    "\n",
    "        # doesn't make sense for learning\n",
    "        if distance < .05: # self.data.time > self.max_target_duration * (self.target_idx + 1):\n",
    "            terminated = True\n",
    "            reward += 1\n",
    "\n",
    "            if self.target_idx < self.max_num_targets - 1:\n",
    "                self.target_idx += 1\n",
    "                self.update_target(self.target_positions[self.target_idx])\n",
    "            else:\n",
    "                done = True\n",
    "\n",
    "        obs = np.concatenate([self.target_positions[self.target_idx], sensor_data])\n",
    "        return obs, reward, done, terminated, {}\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        super().reset(seed=seed)\n",
    "        mujoco.mj_resetData(self.model, self.data)\n",
    "\n",
    "        self.target_positions = self.sample_targets(self.max_num_targets)\n",
    "        self.target_idx = 0\n",
    "        self.update_target(self.target_positions[self.target_idx])\n",
    "\n",
    "        sensor_data = self.data.sensordata.copy()\n",
    "        obs = np.concatenate([self.target_positions[self.target_idx], sensor_data])\n",
    "        return obs, {}\n",
    "\n",
    "    def render(self):\n",
    "        if self.viewer is not None:\n",
    "            self.viewer.sync()\n",
    "        else:\n",
    "            self.viewer = mujoco.viewer.launch_passive(self.model, self.data)\n",
    "            self.viewer.opt.flags[mujoco.mjtVisFlag.mjVIS_JOINT] = True\n",
    "            self.viewer.opt.flags[mujoco.mjtVisFlag.mjVIS_ACTUATOR] = True\n",
    "            self.viewer.cam.lookat[:] = [0, -1.5, -0.5]\n",
    "            self.viewer.cam.azimuth = 90\n",
    "            self.viewer.cam.elevation = 0\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer is not None:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State dimension: 15\n",
      "Action dimension: 4\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# env = gym.make(\"Pendulum-v1\", render_mode=\"human\")\n",
    "env = SequentialReachingEnv(\n",
    "    xml_file=\"arm_model.xml\",\n",
    "    max_num_targets=1,\n",
    "    max_target_duration=3,\n",
    ")\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "print(\"State dimension:\", state_dim)\n",
    "print(\"Action dimension:\", action_dim)\n",
    "\n",
    "hidden_layers = [256, 256]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 001 | Reward: -94.11\n",
      "Episode: 002 | Reward: -139.34\n",
      "Episode: 003 | Reward: -121.23\n",
      "Episode: 004 | Reward: -61.14\n",
      "Episode: 005 | Reward: -121.18\n",
      "Episode: 006 | Reward: -127.99\n"
     ]
    }
   ],
   "source": [
    "# Create SAC agent, replay buffer, and trainer\n",
    "agent = SACAgent(state_dim, action_dim, hidden_layers, device=device)\n",
    "replay_buffer = ReplayBuffer(state_dim, action_dim)\n",
    "trainer = SACTrainer(\n",
    "    env,\n",
    "    agent,\n",
    "    replay_buffer,\n",
    "    batch_size=256,\n",
    "    start_steps=1000,\n",
    "    update_after=1000,\n",
    "    update_every=50,\n",
    "    max_episode_steps=200,\n",
    ")\n",
    "\n",
    "# Run training for a specified number of episodes\n",
    "trainer.run(num_episodes=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v1\", render_mode=\"human\")\n",
    "\n",
    "# uncomment below if you want to visualize the result of the training\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "   env.render()  # Renders the environment window (ensure you have a display)\n",
    "   action = agent.select_action(state, deterministic=False)\n",
    "   next_state, reward, terminated, truncated, info = env.step(action)\n",
    "   done = terminated or truncated\n",
    "   state = next_state\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "animat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
