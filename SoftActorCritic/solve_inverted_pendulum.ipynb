{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import mujoco\n",
    "import mujoco.viewer\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from agent import SACAgent\n",
    "from utils import ReplayBuffer\n",
    "from trainer import SACTrainer\n",
    "\n",
    "# This solve the pendulum environment with soft actor critic algorithm\n",
    "\n",
    "# It uses a bunch of tricks to make this work better:\n",
    "# Twin Q-Networks\n",
    "# Memory Replay (Experience Replay Buffer)\n",
    "# Target Networks with Polyak Averaging\n",
    "# Automatic Entropy Tuning\n",
    "# Reparameterization Trick\n",
    "# Tanh Action Squashing with Log-Probability Correction\n",
    "# Gradient Clipping\n",
    "# Random Action Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialReachingEnv(gym.Env):\n",
    "    \"\"\"Custom 2-Joint Limb with 4 Muscles, 12 Sensors, and a Target Position\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        xml_path=\"path/to/your_model.xml\",\n",
    "        max_num_targets=10,\n",
    "        max_target_duration=3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = mujoco.MjModel.from_xml_path(xml_path)\n",
    "        self.data = mujoco.MjData(self.model)\n",
    "        self.max_num_targets = max_num_targets\n",
    "        self.max_target_duration = max_target_duration\n",
    "        self.viewer = None\n",
    "\n",
    "        num_actuators = self.model.nu\n",
    "\n",
    "        # Define bounds for each sensor\n",
    "        x_low, x_high = -0.7, 0.9\n",
    "        y_low, y_high = -0.9, 0.4\n",
    "        pos_low, pos_high = 0, 1\n",
    "        vel_low, vel_high = -1, 1\n",
    "        frc_low, frc_high = -100, 0\n",
    "\n",
    "        # Observation space: 12 sensor readings + 3D target position\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=np.concatenate(\n",
    "                [\n",
    "                    [x_low, 0, y_low],  # Target position\n",
    "                    np.full(num_actuators, pos_low),  # Actuator positions\n",
    "                    np.full(num_actuators, vel_low),  # Actuator velocities\n",
    "                    np.full(num_actuators, frc_low),  # Actuator forces\n",
    "                ]\n",
    "            ),\n",
    "            high=np.concatenate(\n",
    "                [\n",
    "                    [x_high, 0, y_high],\n",
    "                    np.full(num_actuators, pos_high),\n",
    "                    np.full(num_actuators, vel_high),\n",
    "                    np.full(num_actuators, frc_high),\n",
    "                ]\n",
    "            ),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        # Action space: 4 muscle activations\n",
    "        self.action_space = spaces.Box(low=0.0, high=1.0, shape=(4,), dtype=np.float32)\n",
    "\n",
    "        # Get the site ID using the name of your end effector\n",
    "        self.hand_id = mujoco.mj_name2id(self.model, mujoco.mjtObj.mjOBJ_SITE, \"hand\")\n",
    "\n",
    "        # Parse target positions from CSV file\n",
    "        self.reachable_positions = self.parse_targets(\"../targets.csv\")\n",
    "\n",
    "    def parse_targets(self, targets_path=\"path/to/targets.csv\", bins=100):\n",
    "        target_positions = np.loadtxt(\n",
    "            targets_path, delimiter=\",\", skiprows=1, usecols=(0, 2)\n",
    "        )\n",
    "        x, y = target_positions[:, 0], target_positions[:, 1]\n",
    "        counts2d, x_edges, y_edges = np.histogram2d(x, y, bins=bins)\n",
    "        x_centers = (x_edges[:-1] + x_edges[1:]) / 2\n",
    "        y_centers = (y_edges[:-1] + y_edges[1:]) / 2\n",
    "        nonzero_indices = np.argwhere(counts2d > 0)\n",
    "        return [(x_centers[i], 0, y_centers[j]) for i, j in nonzero_indices]\n",
    "\n",
    "    def sample_targets(self, num_samples=10):\n",
    "        sampled_positions = np.random.choice(\n",
    "            len(self.reachable_positions), num_samples, replace=False\n",
    "        )\n",
    "        return [self.reachable_positions[i] for i in sampled_positions]\n",
    "\n",
    "    def update_target(self, position):\n",
    "        self.data.mocap_pos = position\n",
    "        mujoco.mj_forward(self.model, self.data)\n",
    "\n",
    "    def step(self, action):\n",
    "        self.data.ctrl[:] = action\n",
    "        mujoco.mj_step(self.model, self.data)\n",
    "\n",
    "        sensor_data = self.data.sensordata.copy()\n",
    "        hand_position = self.data.site_xpos[self.hand_id]\n",
    "        distance = np.linalg.norm(\n",
    "            hand_position - self.target_positions[self.target_idx]\n",
    "        )\n",
    "        reward = -distance\n",
    "\n",
    "        done = self.data.time > self.max_target_duration * self.max_num_targets\n",
    "        terminated = False\n",
    "\n",
    "        # doesn't make sense for learning\n",
    "        if self.data.time > self.max_target_duration * (self.target_idx + 1):\n",
    "            terminated = True\n",
    "\n",
    "            if self.target_idx < self.max_num_targets - 1:\n",
    "                self.target_idx += 1\n",
    "                self.update_target(self.target_positions[self.target_idx])\n",
    "            else:\n",
    "                done = True\n",
    "\n",
    "        obs = np.concatenate([self.target_positions[self.target_idx], sensor_data])\n",
    "        return obs, reward, done, terminated, {}\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        super().reset(seed=seed)\n",
    "        mujoco.mj_resetData(self.model, self.data)\n",
    "\n",
    "        self.target_positions = self.sample_targets(self.max_num_targets)\n",
    "        self.target_idx = 0\n",
    "        self.update_target(self.target_positions[self.target_idx])\n",
    "\n",
    "        sensor_data = self.data.sensordata.copy()\n",
    "        obs = np.concatenate([self.target_positions[self.target_idx], sensor_data])\n",
    "        return obs, {}\n",
    "\n",
    "    def render(self):\n",
    "        if self.viewer is not None:\n",
    "            self.viewer.sync()\n",
    "        else:\n",
    "            self.viewer = mujoco.viewer.launch_passive(self.model, self.data)\n",
    "            self.viewer.opt.flags[mujoco.mjtVisFlag.mjVIS_JOINT] = True\n",
    "            self.viewer.opt.flags[mujoco.mjtVisFlag.mjVIS_ACTUATOR] = True\n",
    "            self.viewer.cam.lookat[:] = [0, -1.5, -0.5]\n",
    "            self.viewer.cam.azimuth = 90\n",
    "            self.viewer.cam.elevation = 0\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer is not None:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State dimension: 15\n",
      "Action dimension: 4\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# env = gym.make(\"Pendulum-v1\", render_mode=\"human\")\n",
    "env = SequentialReachingEnv(\n",
    "    xml_path=\"../arm_model.xml\",\n",
    "    max_num_targets=10,\n",
    "    max_target_duration=3,\n",
    ")\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "print(\"State dimension:\", state_dim)\n",
    "print(\"Action dimension:\", action_dim)\n",
    "\n",
    "hidden_layers = [256, 256]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 001 | Reward: -192.95\n",
      "Episode: 002 | Reward: -76.02\n",
      "Episode: 003 | Reward: -87.83\n",
      "Episode: 004 | Reward: -108.49\n",
      "Episode: 005 | Reward: -142.56\n",
      "Episode: 006 | Reward: -78.73\n",
      "Episode: 007 | Reward: -60.25\n",
      "Episode: 008 | Reward: -138.72\n",
      "Episode: 009 | Reward: -160.36\n",
      "Episode: 010 | Reward: -85.00\n",
      "Episode: 011 | Reward: -67.75\n",
      "Episode: 012 | Reward: -161.74\n",
      "Episode: 013 | Reward: -124.49\n",
      "Episode: 014 | Reward: -122.34\n",
      "Episode: 015 | Reward: -176.44\n",
      "Episode: 016 | Reward: -53.87\n",
      "Episode: 017 | Reward: -70.47\n",
      "Episode: 018 | Reward: -114.96\n",
      "Episode: 019 | Reward: -51.90\n",
      "Episode: 020 | Reward: -139.18\n",
      "Episode: 021 | Reward: -83.01\n",
      "Episode: 022 | Reward: -95.74\n",
      "Episode: 023 | Reward: -134.91\n",
      "Episode: 024 | Reward: -70.21\n",
      "Episode: 025 | Reward: -70.84\n",
      "Episode: 026 | Reward: -143.02\n",
      "Episode: 027 | Reward: -60.03\n",
      "Episode: 028 | Reward: -141.16\n",
      "Episode: 029 | Reward: -103.11\n",
      "Episode: 030 | Reward: -131.62\n",
      "Episode: 031 | Reward: -50.27\n",
      "Episode: 032 | Reward: -94.56\n",
      "Episode: 033 | Reward: -151.89\n",
      "Episode: 034 | Reward: -101.81\n",
      "Episode: 035 | Reward: -102.92\n",
      "Episode: 036 | Reward: -84.81\n",
      "Episode: 037 | Reward: -95.77\n",
      "Episode: 038 | Reward: -71.08\n",
      "Episode: 039 | Reward: -132.28\n",
      "Episode: 040 | Reward: -104.53\n",
      "Episode: 041 | Reward: -72.20\n",
      "Episode: 042 | Reward: -94.97\n",
      "Episode: 043 | Reward: -127.84\n",
      "Episode: 044 | Reward: -177.45\n",
      "Episode: 045 | Reward: -179.60\n",
      "Episode: 046 | Reward: -38.46\n",
      "Episode: 047 | Reward: -138.33\n",
      "Episode: 048 | Reward: -95.71\n",
      "Episode: 049 | Reward: -67.33\n",
      "Episode: 050 | Reward: -89.00\n",
      "Episode: 051 | Reward: -107.56\n",
      "Episode: 052 | Reward: -86.66\n",
      "Episode: 053 | Reward: -136.13\n",
      "Episode: 054 | Reward: -163.57\n",
      "Episode: 055 | Reward: -129.51\n",
      "Episode: 056 | Reward: -154.07\n",
      "Episode: 057 | Reward: -85.14\n",
      "Episode: 058 | Reward: -176.20\n",
      "Episode: 059 | Reward: -86.10\n",
      "Episode: 060 | Reward: -138.53\n",
      "Episode: 061 | Reward: -127.80\n",
      "Episode: 062 | Reward: -125.25\n",
      "Episode: 063 | Reward: -50.58\n",
      "Episode: 064 | Reward: -140.53\n",
      "Episode: 065 | Reward: -66.51\n",
      "Episode: 066 | Reward: -147.52\n",
      "Episode: 067 | Reward: -109.45\n",
      "Episode: 068 | Reward: -131.19\n",
      "Episode: 069 | Reward: -71.39\n",
      "Episode: 070 | Reward: -93.92\n",
      "Episode: 071 | Reward: -170.63\n",
      "Episode: 072 | Reward: -56.17\n",
      "Episode: 073 | Reward: -107.94\n",
      "Episode: 074 | Reward: -95.13\n",
      "Episode: 075 | Reward: -94.49\n",
      "Episode: 076 | Reward: -94.77\n",
      "Episode: 077 | Reward: -124.02\n",
      "Episode: 078 | Reward: -94.59\n",
      "Episode: 079 | Reward: -54.81\n",
      "Episode: 080 | Reward: -110.86\n",
      "Episode: 081 | Reward: -91.15\n",
      "Episode: 082 | Reward: -100.68\n",
      "Episode: 083 | Reward: -79.82\n",
      "Episode: 084 | Reward: -103.68\n",
      "Episode: 085 | Reward: -113.07\n",
      "Episode: 086 | Reward: -83.63\n",
      "Episode: 087 | Reward: -159.45\n",
      "Episode: 088 | Reward: -41.02\n",
      "Episode: 089 | Reward: -95.24\n",
      "Episode: 090 | Reward: -99.49\n",
      "Episode: 091 | Reward: -131.42\n",
      "Episode: 092 | Reward: -117.43\n",
      "Episode: 093 | Reward: -154.52\n",
      "Episode: 094 | Reward: -89.90\n",
      "Episode: 095 | Reward: -78.73\n",
      "Episode: 096 | Reward: -167.87\n",
      "Episode: 097 | Reward: -71.51\n",
      "Episode: 098 | Reward: -162.55\n",
      "Episode: 099 | Reward: -95.69\n",
      "Episode: 100 | Reward: -65.08\n",
      "Episode: 101 | Reward: -162.88\n",
      "Episode: 102 | Reward: -169.23\n",
      "Episode: 103 | Reward: -81.05\n",
      "Episode: 104 | Reward: -177.65\n",
      "Episode: 105 | Reward: -96.62\n",
      "Episode: 106 | Reward: -52.93\n",
      "Episode: 107 | Reward: -105.88\n",
      "Episode: 108 | Reward: -49.47\n",
      "Episode: 109 | Reward: -149.52\n",
      "Episode: 110 | Reward: -57.68\n",
      "Episode: 111 | Reward: -122.10\n",
      "Episode: 112 | Reward: -139.22\n",
      "Episode: 113 | Reward: -176.16\n",
      "Episode: 114 | Reward: -186.11\n",
      "Episode: 115 | Reward: -133.39\n",
      "Episode: 116 | Reward: -164.98\n",
      "Episode: 117 | Reward: -78.06\n",
      "Episode: 118 | Reward: -124.78\n",
      "Episode: 119 | Reward: -140.42\n",
      "Episode: 120 | Reward: -94.22\n",
      "Episode: 121 | Reward: -112.79\n",
      "Episode: 122 | Reward: -61.75\n",
      "Episode: 123 | Reward: -65.20\n",
      "Episode: 124 | Reward: -112.42\n",
      "Episode: 125 | Reward: -161.41\n",
      "Episode: 126 | Reward: -122.26\n",
      "Episode: 127 | Reward: -81.50\n",
      "Episode: 128 | Reward: -46.67\n",
      "Episode: 129 | Reward: -154.10\n",
      "Episode: 130 | Reward: -69.50\n",
      "Episode: 131 | Reward: -98.27\n",
      "Episode: 132 | Reward: -135.35\n",
      "Episode: 133 | Reward: -65.45\n",
      "Episode: 134 | Reward: -99.43\n",
      "Episode: 135 | Reward: -61.22\n",
      "Episode: 136 | Reward: -136.87\n",
      "Episode: 137 | Reward: -190.64\n",
      "Episode: 138 | Reward: -94.52\n",
      "Episode: 139 | Reward: -104.41\n",
      "Episode: 140 | Reward: -118.23\n",
      "Episode: 141 | Reward: -129.27\n",
      "Episode: 142 | Reward: -131.58\n",
      "Episode: 143 | Reward: -143.18\n",
      "Episode: 144 | Reward: -103.98\n",
      "Episode: 145 | Reward: -94.35\n",
      "Episode: 146 | Reward: -135.80\n",
      "Episode: 147 | Reward: -82.13\n",
      "Episode: 148 | Reward: -142.25\n",
      "Episode: 149 | Reward: -102.78\n",
      "Episode: 150 | Reward: -167.33\n",
      "Episode: 151 | Reward: -133.45\n",
      "Episode: 152 | Reward: -135.65\n",
      "Episode: 153 | Reward: -118.46\n",
      "Episode: 154 | Reward: -127.43\n",
      "Episode: 155 | Reward: -79.53\n",
      "Episode: 156 | Reward: -120.81\n",
      "Episode: 157 | Reward: -96.92\n",
      "Episode: 158 | Reward: -151.08\n",
      "Episode: 159 | Reward: -75.86\n",
      "Episode: 160 | Reward: -178.35\n",
      "Episode: 161 | Reward: -78.87\n",
      "Episode: 162 | Reward: -158.78\n",
      "Episode: 163 | Reward: -89.60\n",
      "Episode: 164 | Reward: -32.16\n",
      "Episode: 165 | Reward: -140.40\n",
      "Episode: 166 | Reward: -68.28\n",
      "Episode: 167 | Reward: -67.00\n",
      "Episode: 168 | Reward: -169.46\n",
      "Episode: 169 | Reward: -155.02\n",
      "Episode: 170 | Reward: -89.83\n",
      "Episode: 171 | Reward: -118.32\n",
      "Episode: 172 | Reward: -160.02\n",
      "Episode: 173 | Reward: -135.07\n",
      "Episode: 174 | Reward: -60.07\n",
      "Episode: 175 | Reward: -169.21\n",
      "Episode: 176 | Reward: -49.78\n",
      "Episode: 177 | Reward: -66.50\n",
      "Episode: 178 | Reward: -132.63\n",
      "Episode: 179 | Reward: -91.43\n",
      "Episode: 180 | Reward: -125.72\n",
      "Episode: 181 | Reward: -151.73\n",
      "Episode: 182 | Reward: -131.03\n",
      "Episode: 183 | Reward: -101.62\n",
      "Episode: 184 | Reward: -134.86\n",
      "Episode: 185 | Reward: -42.67\n",
      "Episode: 186 | Reward: -56.36\n",
      "Episode: 187 | Reward: -87.81\n",
      "Episode: 188 | Reward: -86.41\n",
      "Episode: 189 | Reward: -114.78\n",
      "Episode: 190 | Reward: -81.50\n",
      "Episode: 191 | Reward: -129.29\n",
      "Episode: 192 | Reward: -127.26\n",
      "Episode: 193 | Reward: -140.31\n",
      "Episode: 194 | Reward: -94.19\n",
      "Episode: 195 | Reward: -77.20\n",
      "Episode: 196 | Reward: -89.98\n",
      "Episode: 197 | Reward: -87.04\n",
      "Episode: 198 | Reward: -155.15\n",
      "Episode: 199 | Reward: -157.79\n",
      "Episode: 200 | Reward: -141.80\n"
     ]
    }
   ],
   "source": [
    "# Create SAC agent, replay buffer, and trainer\n",
    "agent = SACAgent(state_dim, action_dim, hidden_layers, device=device)\n",
    "replay_buffer = ReplayBuffer(state_dim, action_dim)\n",
    "trainer = SACTrainer(\n",
    "    env,\n",
    "    agent,\n",
    "    replay_buffer,\n",
    "    batch_size=256,\n",
    "    start_steps=1000,\n",
    "    update_after=1000,\n",
    "    update_every=50,\n",
    "    max_episode_steps=200,\n",
    ")\n",
    "\n",
    "# Run training for a specified number of episodes\n",
    "trainer.run(num_episodes=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v1\", render_mode=\"human\")\n",
    "\n",
    "# uncomment below if you want to visualize the result of the training\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "   env.render()  # Renders the environment window (ensure you have a display)\n",
    "   action = agent.select_action(state, deterministic=False)\n",
    "   next_state, reward, terminated, truncated, info = env.step(action)\n",
    "   done = terminated or truncated\n",
    "   state = next_state\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "animat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
