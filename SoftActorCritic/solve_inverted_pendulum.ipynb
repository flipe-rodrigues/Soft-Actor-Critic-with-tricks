{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rlab/anaconda3/envs/sac-env/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 001 | Reward: -1349.08\n",
      "Episode: 002 | Reward: -761.96\n",
      "Episode: 003 | Reward: -1279.37\n",
      "Episode: 004 | Reward: -967.42\n",
      "Episode: 005 | Reward: -847.91\n",
      "Episode: 006 | Reward: -860.07\n",
      "Episode: 007 | Reward: -901.13\n",
      "Episode: 008 | Reward: -1747.36\n",
      "Episode: 009 | Reward: -1242.77\n",
      "Episode: 010 | Reward: -1194.34\n",
      "Episode: 011 | Reward: -1292.58\n",
      "Episode: 012 | Reward: -1486.94\n",
      "Episode: 013 | Reward: -1270.05\n",
      "Episode: 014 | Reward: -1111.99\n",
      "Episode: 015 | Reward: -1099.51\n",
      "Episode: 016 | Reward: -770.21\n",
      "Episode: 017 | Reward: -888.82\n",
      "Episode: 018 | Reward: -869.33\n",
      "Episode: 019 | Reward: -858.68\n",
      "Episode: 020 | Reward: -1016.20\n",
      "Episode: 021 | Reward: -993.87\n",
      "Episode: 022 | Reward: -1007.54\n",
      "Episode: 023 | Reward: -1043.51\n",
      "Episode: 024 | Reward: -877.86\n",
      "Episode: 025 | Reward: -385.27\n",
      "Episode: 026 | Reward: -1073.10\n",
      "Episode: 027 | Reward: -1169.34\n",
      "Episode: 028 | Reward: -121.84\n",
      "Episode: 029 | Reward: -1016.30\n",
      "Episode: 030 | Reward: -0.50\n",
      "Episode: 031 | Reward: -1072.29\n",
      "Episode: 032 | Reward: -479.15\n",
      "Episode: 033 | Reward: -123.75\n",
      "Episode: 034 | Reward: -1054.91\n",
      "Episode: 035 | Reward: -1054.48\n",
      "Episode: 036 | Reward: -900.83\n",
      "Episode: 037 | Reward: -362.18\n",
      "Episode: 038 | Reward: -356.33\n",
      "Episode: 039 | Reward: -633.75\n",
      "Episode: 040 | Reward: -364.44\n",
      "Episode: 041 | Reward: -249.23\n",
      "Episode: 042 | Reward: -124.57\n",
      "Episode: 043 | Reward: -248.84\n",
      "Episode: 044 | Reward: -125.30\n",
      "Episode: 045 | Reward: -119.34\n",
      "Episode: 046 | Reward: -533.85\n",
      "Episode: 047 | Reward: -119.35\n",
      "Episode: 048 | Reward: -0.40\n",
      "Episode: 049 | Reward: -1.64\n",
      "Episode: 050 | Reward: -364.98\n",
      "Episode: 051 | Reward: -124.87\n",
      "Episode: 052 | Reward: -508.18\n",
      "Episode: 053 | Reward: -125.96\n",
      "Episode: 054 | Reward: -0.85\n",
      "Episode: 055 | Reward: -122.89\n",
      "Episode: 056 | Reward: -0.57\n",
      "Episode: 057 | Reward: -125.51\n",
      "Episode: 058 | Reward: -609.07\n",
      "Episode: 059 | Reward: -543.71\n",
      "Episode: 060 | Reward: -121.32\n",
      "Episode: 061 | Reward: -121.54\n",
      "Episode: 062 | Reward: -123.58\n",
      "Episode: 063 | Reward: -591.05\n",
      "Episode: 064 | Reward: -124.57\n",
      "Episode: 065 | Reward: -238.80\n",
      "Episode: 066 | Reward: -485.31\n",
      "Episode: 067 | Reward: -0.73\n",
      "Episode: 068 | Reward: -512.15\n",
      "Episode: 069 | Reward: -245.25\n",
      "Episode: 070 | Reward: -497.02\n",
      "Episode: 071 | Reward: -475.06\n",
      "Episode: 072 | Reward: -124.18\n",
      "Episode: 073 | Reward: -120.67\n",
      "Episode: 074 | Reward: -483.21\n",
      "Episode: 075 | Reward: -490.42\n",
      "Episode: 076 | Reward: -532.76\n",
      "Episode: 077 | Reward: -601.37\n",
      "Episode: 078 | Reward: -125.62\n",
      "Episode: 079 | Reward: -477.76\n",
      "Episode: 080 | Reward: -241.13\n",
      "Episode: 081 | Reward: -0.73\n",
      "Episode: 082 | Reward: -125.86\n",
      "Episode: 083 | Reward: -241.45\n",
      "Episode: 084 | Reward: -125.79\n",
      "Episode: 085 | Reward: -245.25\n",
      "Episode: 086 | Reward: -122.93\n",
      "Episode: 087 | Reward: -120.88\n",
      "Episode: 088 | Reward: -234.98\n",
      "Episode: 089 | Reward: -124.75\n",
      "Episode: 090 | Reward: -120.78\n",
      "Episode: 091 | Reward: -1.73\n",
      "Episode: 092 | Reward: -0.14\n",
      "Episode: 093 | Reward: -622.85\n",
      "Episode: 094 | Reward: -124.34\n",
      "Episode: 095 | Reward: -470.40\n",
      "Episode: 096 | Reward: -121.96\n",
      "Episode: 097 | Reward: -125.30\n",
      "Episode: 098 | Reward: -235.20\n",
      "Episode: 099 | Reward: -369.27\n",
      "Episode: 100 | Reward: -122.42\n",
      "Episode: 101 | Reward: -486.66\n",
      "Episode: 102 | Reward: -354.84\n",
      "Episode: 103 | Reward: -123.65\n",
      "Episode: 104 | Reward: -513.44\n",
      "Episode: 105 | Reward: -119.48\n",
      "Episode: 106 | Reward: -481.92\n",
      "Episode: 107 | Reward: -483.76\n",
      "Episode: 108 | Reward: -125.86\n",
      "Episode: 109 | Reward: -661.34\n",
      "Episode: 110 | Reward: -546.87\n",
      "Episode: 111 | Reward: -244.75\n",
      "Episode: 112 | Reward: -119.88\n",
      "Episode: 113 | Reward: -236.84\n",
      "Episode: 114 | Reward: -125.75\n",
      "Episode: 115 | Reward: -120.99\n",
      "Episode: 116 | Reward: -0.35\n",
      "Episode: 117 | Reward: -238.07\n",
      "Episode: 118 | Reward: -485.43\n",
      "Episode: 119 | Reward: -122.68\n",
      "Episode: 120 | Reward: -238.94\n",
      "Episode: 121 | Reward: -530.81\n",
      "Episode: 122 | Reward: -122.71\n",
      "Episode: 123 | Reward: -123.17\n",
      "Episode: 124 | Reward: -242.31\n",
      "Episode: 125 | Reward: -120.94\n",
      "Episode: 126 | Reward: -630.30\n",
      "Episode: 127 | Reward: -123.23\n",
      "Episode: 128 | Reward: -489.10\n",
      "Episode: 129 | Reward: -476.62\n",
      "Episode: 130 | Reward: -0.29\n",
      "Episode: 131 | Reward: -237.43\n",
      "Episode: 132 | Reward: -515.89\n",
      "Episode: 133 | Reward: -124.20\n",
      "Episode: 134 | Reward: -121.14\n",
      "Episode: 135 | Reward: -119.26\n",
      "Episode: 136 | Reward: -126.27\n",
      "Episode: 137 | Reward: -122.27\n",
      "Episode: 138 | Reward: -120.54\n",
      "Episode: 139 | Reward: -122.73\n",
      "Episode: 140 | Reward: -0.53\n",
      "Episode: 141 | Reward: -0.24\n",
      "Episode: 142 | Reward: -363.76\n",
      "Episode: 143 | Reward: -125.41\n",
      "Episode: 144 | Reward: -244.52\n",
      "Episode: 145 | Reward: -364.50\n",
      "Episode: 146 | Reward: -124.77\n",
      "Episode: 147 | Reward: -354.67\n",
      "Episode: 148 | Reward: -490.98\n",
      "Episode: 149 | Reward: -125.32\n",
      "Episode: 150 | Reward: -358.90\n",
      "Episode: 151 | Reward: -124.48\n",
      "Episode: 152 | Reward: -241.95\n",
      "Episode: 153 | Reward: -126.31\n",
      "Episode: 154 | Reward: -125.43\n",
      "Episode: 155 | Reward: -359.64\n",
      "Episode: 156 | Reward: -484.39\n",
      "Episode: 157 | Reward: -512.29\n",
      "Episode: 158 | Reward: -364.44\n",
      "Episode: 159 | Reward: -119.23\n",
      "Episode: 160 | Reward: -619.03\n",
      "Episode: 161 | Reward: -355.63\n",
      "Episode: 162 | Reward: -713.22\n",
      "Episode: 163 | Reward: -120.53\n",
      "Episode: 164 | Reward: -0.23\n",
      "Episode: 165 | Reward: -122.87\n",
      "Episode: 166 | Reward: -355.70\n",
      "Episode: 167 | Reward: -533.33\n",
      "Episode: 168 | Reward: -0.26\n",
      "Episode: 169 | Reward: -123.34\n",
      "Episode: 170 | Reward: -121.01\n",
      "Episode: 171 | Reward: -125.45\n",
      "Episode: 172 | Reward: -125.38\n",
      "Episode: 173 | Reward: -123.47\n",
      "Episode: 174 | Reward: -241.82\n",
      "Episode: 175 | Reward: -610.40\n",
      "Episode: 176 | Reward: -117.89\n",
      "Episode: 177 | Reward: -122.60\n",
      "Episode: 178 | Reward: -0.59\n",
      "Episode: 179 | Reward: -239.72\n",
      "Episode: 180 | Reward: -123.94\n",
      "Episode: 181 | Reward: -120.94\n",
      "Episode: 182 | Reward: -598.68\n",
      "Episode: 183 | Reward: -123.63\n",
      "Episode: 184 | Reward: -380.97\n",
      "Episode: 185 | Reward: -121.22\n",
      "Episode: 186 | Reward: -121.89\n",
      "Episode: 187 | Reward: -241.08\n",
      "Episode: 188 | Reward: -0.15\n",
      "Episode: 189 | Reward: -240.81\n",
      "Episode: 190 | Reward: -354.55\n",
      "Episode: 191 | Reward: -121.78\n",
      "Episode: 192 | Reward: -124.53\n",
      "Episode: 193 | Reward: -125.86\n",
      "Episode: 194 | Reward: -679.77\n",
      "Episode: 195 | Reward: -600.59\n",
      "Episode: 196 | Reward: -120.20\n",
      "Episode: 197 | Reward: -124.81\n",
      "Episode: 198 | Reward: -0.45\n",
      "Episode: 199 | Reward: -119.09\n",
      "Episode: 200 | Reward: -351.48\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gym\n",
    "from agent import SACAgent\n",
    "from utils import ReplayBuffer\n",
    "from trainer import SACTrainer\n",
    "\n",
    "# This solve the pendulum environment with soft actor critic\n",
    "\n",
    "# It uses a bunch of tricks to make this work better:\n",
    "# Twin Q-Networks\n",
    "# Memory Replay (Experience Replay Buffer)\n",
    "Target Networks with Polyak Averaging\n",
    "# Automatic Entropy Tuning\n",
    "# Reparameterization Trick\n",
    "# Tanh Action Squashing with Log-Probability Correction\n",
    "# Gradient Clipping\n",
    "# Random Action Initialization\n",
    "\n",
    "\n",
    "env = gym.make(\"Pendulum-v1\")  \n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "hidden_layers = [256, 256]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create SAC agent, replay buffer, and trainer\n",
    "agent = SACAgent(state_dim, action_dim, hidden_layers, device=device)\n",
    "replay_buffer = ReplayBuffer(state_dim, action_dim)\n",
    "trainer = SACTrainer(env, agent, replay_buffer,\n",
    "                        batch_size=256,\n",
    "                        start_steps=1000,\n",
    "                        update_after=1000,\n",
    "                        update_every=50,\n",
    "                        max_episode_steps=200)\n",
    "\n",
    "# Run training for a specified number of episodes\n",
    "trainer.run(num_episodes=200)\n",
    "\n",
    "# uncomment below if you want to visualize the result of the training\n",
    "#state, _ = env.reset()\n",
    "#done = False\n",
    "#while not done:\n",
    "#    env.render()  # Renders the environment window (ensure you have a display)\n",
    "#    action = agent.select_action(state, deterministic=True)\n",
    "#    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "#    done = terminated or truncated\n",
    "#    state = next_state\n",
    "#env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sac-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
