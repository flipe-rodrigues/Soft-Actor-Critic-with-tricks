{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import mujoco\n",
    "import mujoco.viewer\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from agent import SACAgent\n",
    "from utils import ReplayBuffer\n",
    "from trainer import SACTrainer\n",
    "\n",
    "# This solve the pendulum environment with soft actor critic algorithm\n",
    "\n",
    "# It uses a bunch of tricks to make this work better:\n",
    "# Twin Q-Networks\n",
    "# Memory Replay (Experience Replay Buffer)\n",
    "# Target Networks with Polyak Averaging\n",
    "# Automatic Entropy Tuning\n",
    "# Reparameterization Trick\n",
    "# Tanh Action Squashing with Log-Probability Correction\n",
    "# Gradient Clipping\n",
    "# Random Action Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialReachingEnv(gym.Env):\n",
    "    \"\"\"Custom 2-Joint Limb with 4 Muscles, 12 Sensors, and a Target Position\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        xml_file=\"your_model.xml\",\n",
    "        max_num_targets=10,\n",
    "        max_target_duration=3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        mj_dir = \"../mujoco\"\n",
    "        xml_path = os.path.join(mj_dir, xml_file)\n",
    "        self.model = mujoco.MjModel.from_xml_path(xml_path)\n",
    "        self.data = mujoco.MjData(self.model)\n",
    "        self.max_num_targets = max_num_targets\n",
    "        self.max_target_duration = max_target_duration\n",
    "        self.viewer = None\n",
    "\n",
    "        # Get the site ID using the name of your end effector\n",
    "        self.hand_id = self.model.geom(\"hand\").id\n",
    "        \n",
    "        # Load sensor stats\n",
    "        sensor_stats_path = os.path.join(mj_dir, \"sensor_stats.pkl\")\n",
    "        with open(sensor_stats_path, \"rb\") as f:\n",
    "            self.sensor_stats = pickle.load(f)\n",
    "\n",
    "        # Load target stats\n",
    "        target_stats_path = os.path.join(mj_dir, \"target_stats.pkl\")\n",
    "        with open(target_stats_path, \"rb\") as f:\n",
    "            self.target_stats = pickle.load(f)\n",
    "\n",
    "        # Define the lower and upper bounds for each feature (15 features)\n",
    "        low_values = np.concatenate(\n",
    "            [\n",
    "                self.sensor_stats[\"Min\"].values,\n",
    "                self.target_stats[\"Min\"].values,\n",
    "            ]\n",
    "        )\n",
    "        high_values = np.concatenate(\n",
    "            [\n",
    "                self.sensor_stats[\"Max\"].values,\n",
    "                self.target_stats[\"Max\"].values,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Observation space: 12 sensor readings + 3D target position\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=low_values, high=high_values, dtype=np.float64\n",
    "        )\n",
    "\n",
    "        # Action space: 4 muscle activations\n",
    "        self.action_space = spaces.Box(low=0.0, high=1.0, shape=(4,), dtype=np.float64)\n",
    "\n",
    "        # Load valid target positions\n",
    "        reachable_positions_path = os.path.join(mj_dir, \"reachable_positions.pkl\")\n",
    "        with open(reachable_positions_path, \"rb\") as f:\n",
    "            self.reachable_positions = pickle.load(f)\n",
    "\n",
    "    def sample_targets(self, num_samples=10):\n",
    "        return self.reachable_positions.sample(num_samples).values\n",
    "\n",
    "    def update_target(self, position):\n",
    "        self.data.mocap_pos = position\n",
    "        mujoco.mj_forward(self.model, self.data)\n",
    "\n",
    "    def step(self, action):\n",
    "        self.data.ctrl[:] = action\n",
    "        mujoco.mj_step(self.model, self.data)\n",
    "\n",
    "        sensor_data = self.data.sensordata.copy()\n",
    "        hand_position = self.data.site_xpos[self.hand_id]\n",
    "        distance = np.linalg.norm(\n",
    "            hand_position - self.target_positions[self.target_idx]\n",
    "        )\n",
    "        reward = -distance\n",
    "\n",
    "        done = self.data.time > self.max_target_duration * self.max_num_targets\n",
    "        terminated = False\n",
    "\n",
    "        # doesn't make sense for learning\n",
    "        if distance < .05: # self.data.time > self.max_target_duration * (self.target_idx + 1):\n",
    "            terminated = True\n",
    "            reward += 1\n",
    "\n",
    "            if self.target_idx < self.max_num_targets - 1:\n",
    "                self.target_idx += 1\n",
    "                self.update_target(self.target_positions[self.target_idx])\n",
    "            else:\n",
    "                done = True\n",
    "\n",
    "        obs = np.concatenate([self.target_positions[self.target_idx], sensor_data])\n",
    "        return obs, reward, done, terminated, {}\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        super().reset(seed=seed)\n",
    "        mujoco.mj_resetData(self.model, self.data)\n",
    "\n",
    "        self.target_positions = self.sample_targets(self.max_num_targets)\n",
    "        self.target_idx = 0\n",
    "        self.update_target(self.target_positions[self.target_idx])\n",
    "\n",
    "        sensor_data = self.data.sensordata.copy()\n",
    "        obs = np.concatenate([self.target_positions[self.target_idx], sensor_data])\n",
    "        return obs, {}\n",
    "\n",
    "    def render(self):\n",
    "        if self.viewer is not None:\n",
    "            self.viewer.sync()\n",
    "        else:\n",
    "            self.viewer = mujoco.viewer.launch_passive(self.model, self.data)\n",
    "            self.viewer.opt.flags[mujoco.mjtVisFlag.mjVIS_JOINT] = True\n",
    "            self.viewer.opt.flags[mujoco.mjtVisFlag.mjVIS_ACTUATOR] = True\n",
    "            self.viewer.cam.lookat[:] = [0, -1.5, -0.5]\n",
    "            self.viewer.cam.azimuth = 90\n",
    "            self.viewer.cam.elevation = 0\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer is not None:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State dimension: 15\n",
      "Action dimension: 4\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# env = gym.make(\"Pendulum-v1\", render_mode=\"human\")\n",
    "env = SequentialReachingEnv(\n",
    "    xml_file=\"arm_model.xml\",\n",
    "    max_num_targets=1,\n",
    "    max_target_duration=3,\n",
    ")\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "print(\"State dimension:\", state_dim)\n",
    "print(\"Action dimension:\", action_dim)\n",
    "\n",
    "hidden_layers = [256, 256]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 001 | Reward: -53.45\n",
      "Episode: 002 | Reward: -137.72\n",
      "Episode: 003 | Reward: -110.59\n",
      "Episode: 004 | Reward: -92.98\n",
      "Episode: 005 | Reward: -116.43\n",
      "Episode: 006 | Reward: -122.79\n",
      "Episode: 007 | Reward: -85.61\n",
      "Episode: 008 | Reward: -145.67\n",
      "Episode: 009 | Reward: -113.49\n",
      "Episode: 010 | Reward: -109.33\n",
      "Episode: 011 | Reward: -124.68\n",
      "Episode: 012 | Reward: -120.16\n",
      "Episode: 013 | Reward: -111.44\n",
      "Episode: 014 | Reward: -80.83\n",
      "Episode: 015 | Reward: -138.18\n",
      "Episode: 016 | Reward: -73.16\n",
      "Episode: 017 | Reward: -114.77\n",
      "Episode: 018 | Reward: -150.22\n",
      "Episode: 019 | Reward: -118.44\n",
      "Episode: 020 | Reward: -106.95\n",
      "Episode: 021 | Reward: -115.49\n",
      "Episode: 022 | Reward: -136.87\n",
      "Episode: 023 | Reward: -117.94\n",
      "Episode: 024 | Reward: -152.82\n",
      "Episode: 025 | Reward: -101.49\n",
      "Episode: 026 | Reward: -145.22\n",
      "Episode: 027 | Reward: -130.39\n",
      "Episode: 028 | Reward: -87.01\n",
      "Episode: 029 | Reward: -122.35\n",
      "Episode: 030 | Reward: -145.95\n",
      "Episode: 031 | Reward: -119.64\n",
      "Episode: 032 | Reward: -158.11\n",
      "Episode: 033 | Reward: -105.64\n",
      "Episode: 034 | Reward: -100.24\n",
      "Episode: 035 | Reward: -160.10\n",
      "Episode: 036 | Reward: -50.02\n",
      "Episode: 037 | Reward: -76.65\n",
      "Episode: 038 | Reward: -92.22\n",
      "Episode: 039 | Reward: -70.03\n",
      "Episode: 040 | Reward: -150.76\n",
      "Episode: 041 | Reward: -88.40\n",
      "Episode: 042 | Reward: -133.95\n",
      "Episode: 043 | Reward: -138.59\n",
      "Episode: 044 | Reward: -86.90\n",
      "Episode: 045 | Reward: -145.62\n",
      "Episode: 046 | Reward: -152.06\n",
      "Episode: 047 | Reward: -142.29\n",
      "Episode: 048 | Reward: -154.44\n",
      "Episode: 049 | Reward: -154.98\n",
      "Episode: 050 | Reward: -135.53\n",
      "Episode: 051 | Reward: -158.25\n",
      "Episode: 052 | Reward: -92.01\n",
      "Episode: 053 | Reward: -125.49\n",
      "Episode: 054 | Reward: -151.86\n",
      "Episode: 055 | Reward: -94.08\n",
      "Episode: 056 | Reward: -65.42\n",
      "Episode: 057 | Reward: -132.83\n",
      "Episode: 058 | Reward: -105.98\n",
      "Episode: 059 | Reward: -78.78\n",
      "Episode: 060 | Reward: -102.77\n",
      "Episode: 061 | Reward: -147.58\n",
      "Episode: 062 | Reward: -157.63\n",
      "Episode: 063 | Reward: -109.65\n",
      "Episode: 064 | Reward: -151.48\n",
      "Episode: 065 | Reward: -81.26\n",
      "Episode: 066 | Reward: -69.31\n",
      "Episode: 067 | Reward: -88.73\n",
      "Episode: 068 | Reward: -63.68\n",
      "Episode: 069 | Reward: -74.39\n",
      "Episode: 070 | Reward: -108.95\n",
      "Episode: 071 | Reward: -148.73\n",
      "Episode: 072 | Reward: -113.07\n",
      "Episode: 073 | Reward: -147.23\n",
      "Episode: 074 | Reward: -154.19\n",
      "Episode: 075 | Reward: -65.97\n",
      "Episode: 076 | Reward: -110.21\n",
      "Episode: 077 | Reward: -91.88\n",
      "Episode: 078 | Reward: -107.43\n",
      "Episode: 079 | Reward: -140.42\n",
      "Episode: 080 | Reward: -160.86\n",
      "Episode: 081 | Reward: -85.48\n",
      "Episode: 082 | Reward: -96.45\n",
      "Episode: 083 | Reward: -77.23\n",
      "Episode: 084 | Reward: -139.43\n",
      "Episode: 085 | Reward: -133.77\n",
      "Episode: 086 | Reward: -120.36\n",
      "Episode: 087 | Reward: -93.06\n",
      "Episode: 088 | Reward: -153.05\n",
      "Episode: 089 | Reward: -52.22\n",
      "Episode: 090 | Reward: -111.34\n",
      "Episode: 091 | Reward: -58.02\n",
      "Episode: 092 | Reward: -136.43\n",
      "Episode: 093 | Reward: -152.79\n",
      "Episode: 094 | Reward: -130.08\n",
      "Episode: 095 | Reward: -73.13\n",
      "Episode: 096 | Reward: -160.40\n",
      "Episode: 097 | Reward: -107.61\n",
      "Episode: 098 | Reward: -156.78\n",
      "Episode: 099 | Reward: -146.54\n",
      "Episode: 100 | Reward: -119.63\n",
      "Episode: 101 | Reward: -115.75\n",
      "Episode: 102 | Reward: -88.27\n",
      "Episode: 103 | Reward: -93.99\n",
      "Episode: 104 | Reward: -148.77\n",
      "Episode: 105 | Reward: -112.89\n",
      "Episode: 106 | Reward: -127.13\n",
      "Episode: 107 | Reward: -97.54\n",
      "Episode: 108 | Reward: -95.01\n",
      "Episode: 109 | Reward: -145.74\n",
      "Episode: 110 | Reward: -108.27\n",
      "Episode: 111 | Reward: -73.63\n",
      "Episode: 112 | Reward: -65.34\n",
      "Episode: 113 | Reward: -105.74\n",
      "Episode: 114 | Reward: -116.09\n",
      "Episode: 115 | Reward: -135.96\n",
      "Episode: 116 | Reward: -95.10\n",
      "Episode: 117 | Reward: -138.07\n",
      "Episode: 118 | Reward: -128.88\n",
      "Episode: 119 | Reward: -87.95\n",
      "Episode: 120 | Reward: -44.78\n",
      "Episode: 121 | Reward: -158.77\n",
      "Episode: 122 | Reward: -54.97\n",
      "Episode: 123 | Reward: -139.65\n",
      "Episode: 124 | Reward: -131.30\n",
      "Episode: 125 | Reward: -118.31\n",
      "Episode: 126 | Reward: -132.14\n",
      "Episode: 127 | Reward: -111.26\n",
      "Episode: 128 | Reward: -122.76\n",
      "Episode: 129 | Reward: -133.32\n",
      "Episode: 130 | Reward: -136.24\n",
      "Episode: 131 | Reward: -150.20\n",
      "Episode: 132 | Reward: -117.67\n",
      "Episode: 133 | Reward: -81.46\n",
      "Episode: 134 | Reward: -103.91\n",
      "Episode: 135 | Reward: -57.97\n",
      "Episode: 136 | Reward: -124.60\n",
      "Episode: 137 | Reward: -63.97\n",
      "Episode: 138 | Reward: -79.69\n",
      "Episode: 139 | Reward: -164.95\n",
      "Episode: 140 | Reward: -116.62\n",
      "Episode: 141 | Reward: -151.49\n",
      "Episode: 142 | Reward: -141.82\n",
      "Episode: 143 | Reward: -151.88\n",
      "Episode: 144 | Reward: -106.44\n",
      "Episode: 145 | Reward: -151.12\n",
      "Episode: 146 | Reward: -129.99\n",
      "Episode: 147 | Reward: -143.15\n",
      "Episode: 148 | Reward: -90.29\n",
      "Episode: 149 | Reward: -105.92\n",
      "Episode: 150 | Reward: -152.59\n",
      "Episode: 151 | Reward: -100.67\n",
      "Episode: 152 | Reward: -71.71\n",
      "Episode: 153 | Reward: -111.09\n",
      "Episode: 154 | Reward: -115.58\n",
      "Episode: 155 | Reward: -107.18\n",
      "Episode: 156 | Reward: -60.56\n",
      "Episode: 157 | Reward: -134.73\n",
      "Episode: 158 | Reward: -146.29\n",
      "Episode: 159 | Reward: -101.43\n",
      "Episode: 160 | Reward: -122.81\n",
      "Episode: 161 | Reward: -163.43\n",
      "Episode: 162 | Reward: -151.04\n",
      "Episode: 163 | Reward: -67.98\n",
      "Episode: 164 | Reward: -154.08\n",
      "Episode: 165 | Reward: -139.41\n",
      "Episode: 166 | Reward: -158.21\n",
      "Episode: 167 | Reward: -90.35\n",
      "Episode: 168 | Reward: -127.12\n",
      "Episode: 169 | Reward: -119.06\n",
      "Episode: 170 | Reward: -137.30\n",
      "Episode: 171 | Reward: -116.87\n",
      "Episode: 172 | Reward: -149.11\n",
      "Episode: 173 | Reward: -152.11\n",
      "Episode: 174 | Reward: -147.23\n",
      "Episode: 175 | Reward: -80.22\n",
      "Episode: 176 | Reward: -119.70\n",
      "Episode: 177 | Reward: -147.83\n",
      "Episode: 178 | Reward: -70.89\n",
      "Episode: 179 | Reward: -135.48\n",
      "Episode: 180 | Reward: -113.45\n",
      "Episode: 181 | Reward: -134.26\n",
      "Episode: 182 | Reward: -144.91\n",
      "Episode: 183 | Reward: -119.02\n",
      "Episode: 184 | Reward: -95.70\n",
      "Episode: 185 | Reward: -58.86\n",
      "Episode: 186 | Reward: -101.01\n",
      "Episode: 187 | Reward: -144.19\n",
      "Episode: 188 | Reward: -86.50\n",
      "Episode: 189 | Reward: -142.47\n",
      "Episode: 190 | Reward: -99.92\n",
      "Episode: 191 | Reward: -88.84\n",
      "Episode: 192 | Reward: -163.25\n",
      "Episode: 193 | Reward: -153.54\n",
      "Episode: 194 | Reward: -97.20\n",
      "Episode: 195 | Reward: -79.85\n",
      "Episode: 196 | Reward: -124.94\n",
      "Episode: 197 | Reward: -94.07\n",
      "Episode: 198 | Reward: -146.19\n",
      "Episode: 199 | Reward: -60.18\n",
      "Episode: 200 | Reward: -84.24\n"
     ]
    }
   ],
   "source": [
    "# Create SAC agent, replay buffer, and trainer\n",
    "agent = SACAgent(state_dim, action_dim, hidden_layers, device=device)\n",
    "replay_buffer = ReplayBuffer(state_dim, action_dim)\n",
    "trainer = SACTrainer(\n",
    "    env,\n",
    "    agent,\n",
    "    replay_buffer,\n",
    "    batch_size=256,\n",
    "    start_steps=1000,\n",
    "    update_after=1000,\n",
    "    update_every=50,\n",
    "    max_episode_steps=200,\n",
    ")\n",
    "\n",
    "# Run training for a specified number of episodes\n",
    "trainer.run(num_episodes=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v1\", render_mode=\"human\")\n",
    "\n",
    "# uncomment below if you want to visualize the result of the training\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "   env.render()  # Renders the environment window (ensure you have a display)\n",
    "   action = agent.select_action(state, deterministic=False)\n",
    "   next_state, reward, terminated, truncated, info = env.step(action)\n",
    "   done = terminated or truncated\n",
    "   state = next_state\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "animat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
