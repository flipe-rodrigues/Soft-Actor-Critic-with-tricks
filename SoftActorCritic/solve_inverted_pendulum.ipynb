{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import mujoco\n",
    "import mujoco.viewer\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from agent import SACAgent\n",
    "from utils import ReplayBuffer\n",
    "from trainer import SACTrainer\n",
    "\n",
    "# This solve the pendulum environment with soft actor critic algorithm\n",
    "\n",
    "# It uses a bunch of tricks to make this work better:\n",
    "# Twin Q-Networks\n",
    "# Memory Replay (Experience Replay Buffer)\n",
    "# Target Networks with Polyak Averaging\n",
    "# Automatic Entropy Tuning\n",
    "# Reparameterization Trick\n",
    "# Tanh Action Squashing with Log-Probability Correction\n",
    "# Gradient Clipping\n",
    "# Random Action Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialReachingEnv(gym.Env):\n",
    "    \"\"\"Custom 2-Joint Limb with 4 Muscles, 12 Sensors, and a Target Position\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        xml_path=\"path/to/your_model.xml\",\n",
    "        max_num_targets=10,\n",
    "        max_target_duration=3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = mujoco.MjModel.from_xml_path(xml_path)\n",
    "        self.data = mujoco.MjData(self.model)\n",
    "        self.max_num_targets = max_num_targets\n",
    "        self.max_target_duration = max_target_duration\n",
    "        self.viewer = None\n",
    "\n",
    "        num_actuators = self.model.nu\n",
    "\n",
    "        # Define bounds for each sensor\n",
    "        x_low, x_high = -1, 1\n",
    "        y_low, y_high = -1, 1\n",
    "        pos_low, pos_high = 0, 1\n",
    "        vel_low, vel_high = -1, 1\n",
    "        frc_low, frc_high = -100, 0\n",
    "\n",
    "        # Observation space: 12 sensor readings + 3D target position\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=np.concatenate(\n",
    "                [\n",
    "                    [x_low, 0, y_low],  # Target position\n",
    "                    np.full(num_actuators, pos_low),  # Actuator positions\n",
    "                    np.full(num_actuators, vel_low),  # Actuator velocities\n",
    "                    np.full(num_actuators, frc_low),  # Actuator forces\n",
    "                ]\n",
    "            ),\n",
    "            high=np.concatenate(\n",
    "                [\n",
    "                    [x_high, 0, y_high],\n",
    "                    np.full(num_actuators, pos_high),\n",
    "                    np.full(num_actuators, vel_high),\n",
    "                    np.full(num_actuators, frc_high),\n",
    "                ]\n",
    "            ),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        # Action space: 4 muscle activations\n",
    "        self.action_space = spaces.Box(low=0.0, high=1.0, shape=(4,), dtype=np.float32)\n",
    "\n",
    "        # Get the site ID using the name of your end effector\n",
    "        self.hand_id = mujoco.mj_name2id(self.model, mujoco.mjtObj.mjOBJ_SITE, \"hand\")\n",
    "\n",
    "        # Parse target positions from CSV file\n",
    "        self.reachable_positions = self.parse_targets(\"../targets.csv\")\n",
    "\n",
    "    def parse_targets(self, targets_path=\"path/to/targets.csv\", bins=100):\n",
    "        target_positions = np.loadtxt(\n",
    "            targets_path, delimiter=\",\", skiprows=1, usecols=(0, 2)\n",
    "        )\n",
    "        x, y = target_positions[:, 0], target_positions[:, 1]\n",
    "        counts2d, x_edges, y_edges = np.histogram2d(x, y, bins=bins)\n",
    "        x_centers = (x_edges[:-1] + x_edges[1:]) / 2\n",
    "        y_centers = (y_edges[:-1] + y_edges[1:]) / 2\n",
    "        nonzero_indices = np.argwhere(counts2d > 0)\n",
    "        return [(x_centers[i], 0, y_centers[j]) for i, j in nonzero_indices]\n",
    "\n",
    "    def sample_targets(self, num_samples=10):\n",
    "        sampled_positions = np.random.choice(\n",
    "            len(self.reachable_positions), num_samples, replace=False\n",
    "        )\n",
    "        return [self.reachable_positions[i] for i in sampled_positions]\n",
    "\n",
    "    def update_target(self, position):\n",
    "        self.data.mocap_pos = position\n",
    "        mujoco.mj_forward(self.model, self.data)\n",
    "\n",
    "    def step(self, action):\n",
    "        self.data.ctrl[:] = action\n",
    "        mujoco.mj_step(self.model, self.data)\n",
    "\n",
    "        sensor_data = self.data.sensordata.copy()\n",
    "        hand_position = self.data.site_xpos[self.hand_id]\n",
    "        distance = np.linalg.norm(\n",
    "            hand_position - self.target_positions[self.target_idx]\n",
    "        )\n",
    "        reward = -distance * self.model.opt.timestep\n",
    "\n",
    "        done = self.data.time > self.max_target_duration * self.max_num_targets\n",
    "        if distance < 0.05 or self.data.time > self.max_target_duration * (\n",
    "            self.target_idx + 1\n",
    "        ):\n",
    "            if self.target_idx < self.max_num_targets - 1:\n",
    "                self.target_idx += 1\n",
    "                self.update_target(self.target_positions[self.target_idx])\n",
    "            else:\n",
    "                done = True\n",
    "\n",
    "        obs = np.concatenate([self.target_positions[self.target_idx], sensor_data])\n",
    "        return obs, reward, done, False, {}\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        super().reset(seed=seed)\n",
    "        mujoco.mj_resetData(self.model, self.data)\n",
    "\n",
    "        self.target_positions = self.sample_targets(self.max_num_targets)\n",
    "        self.target_idx = 0\n",
    "        self.update_target(self.target_positions[self.target_idx])\n",
    "\n",
    "        sensor_data = self.data.sensordata.copy()\n",
    "        obs = np.concatenate([self.target_positions[self.target_idx], sensor_data])\n",
    "        return obs, {}\n",
    "\n",
    "    def render(self):\n",
    "        if self.viewer is not None:\n",
    "            self.viewer.sync()\n",
    "        else:\n",
    "            self.viewer = mujoco.viewer.launch_passive(self.model, self.data)\n",
    "            self.viewer.opt.flags[mujoco.mjtVisFlag.mjVIS_JOINT] = True\n",
    "            self.viewer.opt.flags[mujoco.mjtVisFlag.mjVIS_ACTUATOR] = True\n",
    "            self.viewer.cam.lookat[:] = [0, -1.5, -0.5]\n",
    "            self.viewer.cam.azimuth = 90\n",
    "            self.viewer.cam.elevation = 0\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer is not None:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State dimension: 15\n",
      "Action dimension: 4\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# env = gym.make(\"Pendulum-v1\")\n",
    "env = SequentialReachingEnv(\n",
    "    xml_path=\"../arm_model.xml\",\n",
    "    max_num_targets=10,\n",
    "    max_target_duration=3,\n",
    ")\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "print(\"State dimension:\", state_dim)\n",
    "print(\"Action dimension:\", action_dim)\n",
    "\n",
    "hidden_layers = [25, 25]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 001 | Reward: -0.24\n",
      "Episode: 002 | Reward: -0.18\n",
      "Episode: 003 | Reward: -0.31\n",
      "Episode: 004 | Reward: -0.27\n",
      "Episode: 005 | Reward: -0.18\n",
      "Episode: 006 | Reward: -0.17\n",
      "Episode: 007 | Reward: -0.22\n",
      "Episode: 008 | Reward: -0.15\n",
      "Episode: 009 | Reward: -0.17\n",
      "Episode: 010 | Reward: -0.30\n"
     ]
    }
   ],
   "source": [
    "# Create SAC agent, replay buffer, and trainer\n",
    "agent = SACAgent(state_dim, action_dim, hidden_layers, device=device)\n",
    "replay_buffer = ReplayBuffer(state_dim, action_dim)\n",
    "trainer = SACTrainer(\n",
    "    env,\n",
    "    agent,\n",
    "    replay_buffer,\n",
    "    batch_size=256,\n",
    "    start_steps=1000,\n",
    "    update_after=1000,\n",
    "    update_every=50,\n",
    "    max_episode_steps=200,\n",
    ")\n",
    "\n",
    "# Run training for a specified number of episodes\n",
    "trainer.run(num_episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below if you want to visualize the result of the training\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "   env.render()  # Renders the environment window (ensure you have a display)\n",
    "   action = agent.select_action(state, deterministic=True)\n",
    "   next_state, reward, terminated, truncated, info = env.step(action)\n",
    "   done = terminated or truncated\n",
    "   state = next_state\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "animat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
