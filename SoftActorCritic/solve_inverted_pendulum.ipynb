{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import mujoco\n",
    "import mujoco.viewer\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from agent import SACAgent\n",
    "from utils import ReplayBuffer\n",
    "from trainer import SACTrainer\n",
    "\n",
    "# This solve the pendulum environment with soft actor critic algorithm\n",
    "\n",
    "# It uses a bunch of tricks to make this work better:\n",
    "# Twin Q-Networks\n",
    "# Memory Replay (Experience Replay Buffer)\n",
    "# Target Networks with Polyak Averaging\n",
    "# Automatic Entropy Tuning\n",
    "# Reparameterization Trick\n",
    "# Tanh Action Squashing with Log-Probability Correction\n",
    "# Gradient Clipping\n",
    "# Random Action Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialReachingEnv(gym.Env):\n",
    "    \"\"\"Custom 2-Joint Limb with 4 Muscles, 12 Sensors, and a Target Position\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        xml_file=\"your_model.xml\",\n",
    "        max_num_targets=10,\n",
    "        max_target_duration=3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        mj_dir = \"../mujoco\"\n",
    "        xml_path = os.path.join(mj_dir, xml_file)\n",
    "        self.model = mujoco.MjModel.from_xml_path(xml_path)\n",
    "        self.data = mujoco.MjData(self.model)\n",
    "        self.max_num_targets = max_num_targets\n",
    "        self.max_target_duration = max_target_duration\n",
    "        self.viewer = None\n",
    "\n",
    "        # Get the site ID using the name of your end effector\n",
    "        self.hand_id = self.model.geom(\"hand\").id\n",
    "        \n",
    "        # Load sensor stats\n",
    "        sensor_stats_path = os.path.join(mj_dir, \"sensor_stats.pkl\")\n",
    "        with open(sensor_stats_path, \"rb\") as f:\n",
    "            self.sensor_stats = pickle.load(f)\n",
    "\n",
    "        # Load target stats\n",
    "        target_stats_path = os.path.join(mj_dir, \"target_stats.pkl\")\n",
    "        with open(target_stats_path, \"rb\") as f:\n",
    "            self.target_stats = pickle.load(f)\n",
    "\n",
    "        # Define the lower and upper bounds for each feature (15 features)\n",
    "        low_values = np.concatenate(\n",
    "            [\n",
    "                self.sensor_stats[\"Min\"].values,\n",
    "                self.target_stats[\"Min\"].values,\n",
    "            ]\n",
    "        )\n",
    "        high_values = np.concatenate(\n",
    "            [\n",
    "                self.sensor_stats[\"Max\"].values,\n",
    "                self.target_stats[\"Max\"].values,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Observation space: 12 sensor readings + 3D target position\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=low_values, high=high_values, dtype=np.float64\n",
    "        )\n",
    "\n",
    "        # Action space: 4 muscle activations\n",
    "        self.action_space = spaces.Box(low=0.0, high=1.0, shape=(4,), dtype=np.float64)\n",
    "\n",
    "        # Load valid target positions\n",
    "        reachable_positions_path = os.path.join(mj_dir, \"reachable_positions.pkl\")\n",
    "        with open(reachable_positions_path, \"rb\") as f:\n",
    "            self.reachable_positions = pickle.load(f)\n",
    "\n",
    "    def sample_targets(self, num_samples=10):\n",
    "        return self.reachable_positions.sample(num_samples).values\n",
    "\n",
    "    def update_target(self, position):\n",
    "        self.data.mocap_pos = position\n",
    "        mujoco.mj_forward(self.model, self.data)\n",
    "\n",
    "    def step(self, action):\n",
    "        self.data.ctrl[:] = action\n",
    "        mujoco.mj_step(self.model, self.data)\n",
    "\n",
    "        sensor_data = self.data.sensordata.copy()\n",
    "        hand_position = self.data.site_xpos[self.hand_id]\n",
    "        distance = np.linalg.norm(\n",
    "            hand_position - self.target_positions[self.target_idx]\n",
    "        )\n",
    "        reward = -distance\n",
    "\n",
    "        done = self.data.time > self.max_target_duration * self.max_num_targets\n",
    "        terminated = False\n",
    "\n",
    "        # doesn't make sense for learning\n",
    "        if distance < .05: # self.data.time > self.max_target_duration * (self.target_idx + 1):\n",
    "            terminated = True\n",
    "            # reward += 1\n",
    "\n",
    "            if self.target_idx < self.max_num_targets - 1:\n",
    "                self.target_idx += 1\n",
    "                self.update_target(self.target_positions[self.target_idx])\n",
    "            else:\n",
    "                done = True\n",
    "\n",
    "        obs = np.concatenate([self.target_positions[self.target_idx], sensor_data])\n",
    "        return obs, reward, done, terminated, {}\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        super().reset(seed=seed)\n",
    "        mujoco.mj_resetData(self.model, self.data)\n",
    "\n",
    "        self.target_positions = self.sample_targets(self.max_num_targets)\n",
    "        self.target_idx = 0\n",
    "        self.update_target(self.target_positions[self.target_idx])\n",
    "\n",
    "        sensor_data = self.data.sensordata.copy()\n",
    "        obs = np.concatenate([self.target_positions[self.target_idx], sensor_data])\n",
    "        return obs, {}\n",
    "\n",
    "    def render(self):\n",
    "        if self.viewer is not None:\n",
    "            self.viewer.sync()\n",
    "        else:\n",
    "            self.viewer = mujoco.viewer.launch_passive(self.model, self.data)\n",
    "            self.viewer.opt.flags[mujoco.mjtVisFlag.mjVIS_JOINT] = True\n",
    "            self.viewer.opt.flags[mujoco.mjtVisFlag.mjVIS_ACTUATOR] = True\n",
    "            self.viewer.cam.lookat[:] = [0, -1.5, -0.5]\n",
    "            self.viewer.cam.azimuth = 90\n",
    "            self.viewer.cam.elevation = 0\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer is not None:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State dimension: 15\n",
      "Action dimension: 4\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# env = gym.make(\"Pendulum-v1\", render_mode=\"human\")\n",
    "env = SequentialReachingEnv(\n",
    "    xml_file=\"arm_model.xml\",\n",
    "    max_num_targets=1,\n",
    "    max_target_duration=3,\n",
    ")\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "print(\"State dimension:\", state_dim)\n",
    "print(\"Action dimension:\", action_dim)\n",
    "\n",
    "hidden_layers = [256, 256]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "np.int32(1905)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\flipe\\anaconda3\\envs\\animat\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: np.int32(1905)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      4\u001b[39m trainer = SACTrainer(\n\u001b[32m      5\u001b[39m     env,\n\u001b[32m      6\u001b[39m     agent,\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m     max_episode_steps=\u001b[32m200\u001b[39m,\n\u001b[32m     13\u001b[39m )\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Run training for a specified number of episodes\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\flipe\\Documents\\GitHub\\Soft-Actor-Critic-with-tricks\\SoftActorCritic\\trainer.py:27\u001b[39m, in \u001b[36mSACTrainer.run\u001b[39m\u001b[34m(self, num_episodes)\u001b[39m\n\u001b[32m     25\u001b[39m total_steps = \u001b[32m0\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     state, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     episode_reward = \u001b[32m0\u001b[39m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.max_episode_steps):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 116\u001b[39m, in \u001b[36mSequentialReachingEnv.reset\u001b[39m\u001b[34m(self, seed)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28msuper\u001b[39m().reset(seed=seed)\n\u001b[32m    114\u001b[39m mujoco.mj_resetData(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.data)\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m \u001b[38;5;28mself\u001b[39m.target_positions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msample_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_num_targets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[38;5;28mself\u001b[39m.target_idx = \u001b[32m0\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28mself\u001b[39m.update_target(\u001b[38;5;28mself\u001b[39m.target_positions[\u001b[38;5;28mself\u001b[39m.target_idx])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36mSequentialReachingEnv.sample_targets\u001b[39m\u001b[34m(self, num_samples)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msample_targets\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_samples=\u001b[32m10\u001b[39m):\n\u001b[32m     75\u001b[39m     sampled_positions = np.random.choice(\n\u001b[32m     76\u001b[39m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.reachable_positions), num_samples, replace=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     77\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreachable_positions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m sampled_positions]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\flipe\\anaconda3\\envs\\animat\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\flipe\\anaconda3\\envs\\animat\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: np.int32(1905)"
     ]
    }
   ],
   "source": [
    "# Create SAC agent, replay buffer, and trainer\n",
    "agent = SACAgent(state_dim, action_dim, hidden_layers, device=device)\n",
    "replay_buffer = ReplayBuffer(state_dim, action_dim)\n",
    "trainer = SACTrainer(\n",
    "    env,\n",
    "    agent,\n",
    "    replay_buffer,\n",
    "    batch_size=256,\n",
    "    start_steps=1000,\n",
    "    update_after=1000,\n",
    "    update_every=50,\n",
    "    max_episode_steps=200,\n",
    ")\n",
    "\n",
    "# Run training for a specified number of episodes\n",
    "trainer.run(num_episodes=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v1\", render_mode=\"human\")\n",
    "\n",
    "# uncomment below if you want to visualize the result of the training\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "   env.render()  # Renders the environment window (ensure you have a display)\n",
    "   action = agent.select_action(state, deterministic=False)\n",
    "   next_state, reward, terminated, truncated, info = env.step(action)\n",
    "   done = terminated or truncated\n",
    "   state = next_state\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "animat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
